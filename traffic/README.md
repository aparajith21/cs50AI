In the first attempt, I used a pool size of (6, 6), a drop out of 0.5 and 2 dense layers with relu activation and another with softmax activation. Achieved an accuracy of around 84% after 10 epochs on the test set. (loss: 0.5415)
In the second attempt, I changed the pool size to (3, 3) and increased drop out to 0.6. This resulted in a drastic increase in accuracy to 92%.(loss: 0.2989)
Now, I'm going to reduce the drop out to 0.4 and keep the rest same to see how this affects accuracy. Accuracy mildly changed to   93.75% so drop out is going to remain at 0.4.(loss: 0.2831)
In the 4th iteration, changing up the dense layers from (NUM_CATEGORIES * 20, activation = "relu") and (NUM_CATEGORIES * 10, activation = "relu") to (NUM_CATEGORIES * 20, activation = "relu") and (NUM_CATEGORIES * 20, activation = "relu"). This resulted in little change to the accuracy to make it 93.68% (loss: 0.2898)
Again, I'm now changing up the dense layers further to (NUM_CATEGORIES * 40, activation = "relu") and (NUM_CATEGORIES * 40, activation = "relu"). This got the highest training set accuracy after 10 epochs of 92.15% but the test set accuracy reduced to 91.68% (loss: 0.3531) possibly due to overfitting witht he dense layers. Another thing to note is that the program runs really slowly thanks to the additional computation.
Again, I'm changing up the dense layers to (NUM_CATEGORIES * 30, activation = "relu") and (NUM_CATEGORIES * 30, activation = "relu"). This resulted in a test set accuracy of 93.77% which is an improvement. (loss: 0.2750)
Now I'm changing the dense layers to (NUM_CATEGORIES * 30, activation = "relu") and (NUM_CATEGORIES * 15, activation = "relu"). Test set accuracy became 92.92% (loss: 0.3327)
In this iteration, I'm removing the second dense layer. Achieved a test set accuracy of 91.54% (loss: 0.4856)
Now I'm reinstating the 2nd dense layer to get these (NUM_CATEGORIES * 20, activation = "relu") and (NUM_CATEGORIES * 20, activation = "relu")  and increasing filters to 30 .
This lead to the highest test set accuracy of 94.82% and a loss of 0.2171 so this is the final version I'm submitting (of course, I will continue experimenting, but this one is hopefully sufficient for submisison).
